models:
  # Minimal always-on catalog entry.
  # NOTE: Repo does not ship weights; place a GGUF at the specified path to enable real inference.
  - id: "test-mini"
    enabled: true
    roles: ["chat", "code"]
    supported_hardware: ["gpu-cuda", "gpu", "npu", "cpu"]
    min_profile: "light"
    max_profile: "light"
    priority: 100
    path: "models/Phi-3-mini-4k-instruct-q4.gguf"
    download_url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"

  - id: "qwen2.5-coder-7b-instruct"
    enabled: true
    roles: ["chat", "code"]
    supported_hardware: ["gpu-cuda", "gpu", "cpu"]
    min_profile: "medium"
    max_profile: "heavy"
    priority: 10
    path: "models/qwen2.5-coder-7b-instruct.Q4_K_M.gguf"
    download_url: "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf"

  # Future schema examples (disabled until weights exist).
  - id: "llama-3-8b-instruct"
    enabled: false
    roles: ["chat", "code"]
    supported_hardware: ["gpu-cuda", "gpu", "cpu"]
    min_profile: "medium"
    max_profile: "heavy"
    priority: 10
    path: "models/llama-3-8b-instruct.gguf"

  - id: "llama-3-70b-instruct"
    enabled: false
    roles: ["chat", "code"]
    supported_hardware: ["gpu-cuda", "gpu"]
    min_profile: "heavy"
    max_profile: "heavy"
    priority: 20
    path: "models/llama-3-70b-instruct.gguf"

  - id: "whisper-stt"
    enabled: false
    roles: ["stt"]
    supported_hardware: ["gpu-cuda", "gpu", "cpu", "npu"]
    min_profile: "light"
    max_profile: "heavy"
    priority: 50
    path: "models/whisper.gguf"

  - id: "piper-tts"
    enabled: false
    roles: ["tts"]
    supported_hardware: ["cpu", "gpu", "npu"]
    min_profile: "light"
    max_profile: "heavy"
    priority: 60
    path: "models/piper.onnx"