# JARVISv5 Roadmap (Revised)

**Basis**: Current repo state as of 2026-02-16  
**Constraint**: No mocks, no placeholders. Every milestone produces validated, working behavior.

---

## What Already Exists (Do Not Rebuild)

- FSM controller with full state transitions
- ControllerService wiring FSM to workflow nodes
- Workflow nodes: Router, ContextBuilder, LLMWorker, Validator
- Memory system: Working State (JSON), Episodic Trace (SQLite), Semantic Store (FAISS)
- MemoryManager unified interface
- Hardware profiler and model registry
- LocalInferenceClient (llama_cpp wrapper)
- Docker multi-stage build with llama-cpp-python compiled
- Config system (Pydantic settings)
- Validation harness (scripts/validate_backend.py)
- Unit tests for all above components

---

## Milestone 1: Close the Entry Point

**Status**: COMPLETE (already implemented in repo state).

**What**: Wire `backend/api/main.py` to accept user input and return a response through the existing controller stack.

**Why first**: Everything in the repo is built but unreachable. This was the missing connection.

**Tasks**:
1. Expand `Settings` with model path, data paths, and port
2. Add `POST /task` endpoint that calls `ControllerService.run(user_input)`
3. Return `task_id`, `final_state`, `llm_output` from controller result
4. Add `GET /task/{task_id}` to retrieve working state from MemoryManager

**Validation**:
```bash
docker compose up -d
curl -X POST http://localhost:8000/task -d '{"input": "hello"}'
# Returns task_id and response (empty llm_output expected - no model yet)
curl http://localhost:8000/task/{task_id}
```

**Done when**: A message travels in, passes through FSM+nodes, and a structured response comes out - with or without a model loaded.

**Repo Evidence**:
- `backend/api/main.py` implements `POST /task` and `GET /task/{task_id}`.
- `tests/unit/test_api_entrypoint.py` validates the entrypoint shapes.

---

## Milestone 2: Working LLM Response

**What**: Load a real GGUF model and produce actual LLM output through the existing LLMWorkerNode.

**Why here**: Milestone 1 proves the path. This proves the path produces real value.

**Tasks**:
1. Model download script (`scripts/download_models.sh` / `.ps1`) - env-driven, no hardcoded URLs
2. Add `MODEL_PATH` to Settings and `.env.example`
3. Update `LLMWorkerNode` to read model path from settings (currently hardcoded)
4. Update `ControllerService` or `LLMWorkerNode` to initialize model once at startup, not per request
5. Add integration test: submit task → verify `llm_output` is non-empty string

**Validation**:
```bash
# Download model
./scripts/download_models.sh
docker compose up -d
curl -X POST http://localhost:8000/task -d '{"input": "What is 2+2?"}'
# Returns actual LLM-generated text in llm_output
```

**Done when**: Real LLM text returned via the API endpoint.

---

## Milestone 3: Frontend Chat Interface

**What**: Replace the health-check stub in `App.jsx` with a functional chat UI that submits tasks and displays responses.

**Why here**: LLM is working. The user needs a surface to interact with it.

**Tasks**:
1. Chat component: message input, send button, message list
2. API client in `frontend/src/api/` calling `POST /task` and `GET /task/{task_id}`
3. Poll or wait for task completion, display `llm_output`
4. Apply Project.md visual design (navy/cyan palette, message bubbles)
5. Update frontend Dockerfile and docker-compose port (3000 per .env.example)

**Validation**:
```bash
docker compose up -d
# Open http://localhost:3000
# Type message, send, verify LLM response displayed
```

**Done when**: User can chat with local LLM through the browser.

---

## Milestone 4: Persistent Memory in the Chat Path

**What**: Surface episodic trace and working state into the ContextBuilderNode so the LLM receives relevant prior context on each request.

**Why here**: The memory system is built but `ContextBuilderNode` is not yet retrieving anything meaningful. This makes the system actually useful.

**Tasks**:
1. Update `ContextBuilderNode` to query MemoryManager for prior exchanges relevant to current input
2. Format retrieved context into the prompt passed to `LLMWorkerNode`
3. Add `GET /memory/search?q=` endpoint exposing semantic search
4. Add conversation history display to frontend (session continuity)

**Validation**:
```bash
# Submit two related tasks
curl -X POST http://localhost:8000/task -d '{"input": "My name is Alice"}'
curl -X POST http://localhost:8000/task -d '{"input": "What is my name?"}'
# Second response references Alice
curl http://localhost:8000/memory/search?q=name
```

**Done when**: LLM responses reflect prior conversation context.

---

## Milestone 5: Tool Execution (File Read)

**What**: Implement the tool registry and `read_file` tool so the LLM can access local files.

**Why here**: Completes the core daily-use loop (chat + memory + file access). `backend/tools/` is currently empty.

**Tasks**:
1. Implement `backend/tools/registry.py` with JSON schema validation
2. Implement `read_file` tool with path safety constraints
3. Add `ToolCallNode` to workflow nodes
4. Wire `ControllerService` to invoke tools when `RouterNode` classifies intent as tool use
5. Log all tool calls to EpisodicMemory

**Validation**:
```bash
echo "Project status: active" > /tmp/test.txt
curl -X POST http://localhost:8000/task -d '{"input": "Read /tmp/test.txt"}'
# Response contains file contents
# Episodic trace shows tool_call entry
```

**Done when**: LLM can read a local file and return its contents.

---

## Milestone 6: Validation Harness and Regression Suite

**What**: Build out `scripts/validate_backend.py` and `tests/integration/` so the system is regression-safe before adding more complexity.

**Why here**: Milestones 1-5 deliver a working system. Before voice, search, or learning, establish a test floor.

**Tasks**:
1. Integration tests for full request path (Milestone 1-5 behaviors)
2. Agentic tests: 10+ task scenarios with expected output shapes
3. Validate `scripts/validate_backend.py` runs all suites and produces a report in `reports/`
4. Add Docker compose validation service (runs harness on `docker compose up`)

**Validation**:
```bash
docker compose run backend python scripts/validate_backend.py --scope all
# All suites pass, report written to reports/
```

**Done when**: Full regression suite passes and is runnable in Docker.

---

## Milestone 7: API Surface Completion

**What**: Complete the HTTP surface per Project.md Section 3.5 and 5.2 — all endpoints needed for UI, memory controls, and status.

**Why here**: System is proven working. Now expose it properly.

**Tasks**:
1. `GET /health` — already exists, extend with component status
2. `POST /v1/task` — versioned, replaces `/task`
3. `GET /v1/task/{task_id}` — status + artifacts
4. `GET /v1/memory/search` — semantic query
5. `GET /v1/hardware` — hardware profile and state
6. WebSocket `WS /v1/task/{task_id}/stream` — real-time state transitions
7. API key middleware (basic, config-driven)

**Validation**:
```bash
# All endpoints return correct shapes
# WebSocket streams FSM state transitions
# Invalid API key returns 401
```

**Done when**: All endpoints functional with documented contracts.

---

## Milestone 8: Frontend Completion

**What**: Build out the full frontend per Project.md Section 11 — workflow visualizer, settings panel, status indicators.

**Why here**: API surface is stable. Now build the full UI on top of it.

**Tasks**:
1. Workflow visualizer: live FSM state display per task
2. Settings panel: hardware profile, model path, privacy level
3. Status header: backend health, model loaded state, hardware tier
4. Voice panel: layout only (controls visible, non-functional until Milestone 9)
5. Memory controls: search, view history

**Validation**:
```bash
docker compose up -d
# Full UI renders
# Workflow states animate during task execution
# Settings persist to .env / config
```

**Done when**: Full UI functional against the completed API surface.

---

## Milestone 9: Voice System (Optional, Evidence-Gated)

**Proceed only if**: Daily use of Milestones 1-8 shows voice interaction is needed.

**What**: STT/TTS/wake word via `backend/voice/`

**Tasks**:
1. Whisper STT integration
2. Piper TTS integration
3. openWakeWord activation
4. Voice workflow path through ControllerService
5. Frontend voice panel activation

**Validation**: Audio in → task submitted → audio response returned.

---

## Milestone 10: Security and Privacy Layer (Evidence-Gated)

**Proceed only if**: System is in daily use and external API calls or sensitive data are present.

**What**: Populate `backend/security/` with redaction, guardrails, and budget governance.

**Tasks**:
1. PII redaction before any external call
2. Path safety and prompt injection guardrails
3. Budget tracking for any cloud escalation
4. Privacy level enforcement per config

**Validation**: Sensitive input redacted before logging. Budget limit blocks escalation.

---

## Summary

| Milestone | Builds On | Delivers |
|-----------|-----------|----------|
| 1. Entry Point | Existing controller | Message in, response out via API |
| 2. Real LLM | Milestone 1 | Actual AI responses |
| 3. Frontend Chat | Milestone 2 | Browser-based chat |
| 4. Memory Context | Milestone 3 | Conversation continuity |
| 5. Tool Execution | Milestone 4 | File reading via LLM |
| 6. Regression Suite | Milestone 5 | Safe baseline for future work |
| 7. API Completion | Milestone 6 | Full HTTP/WebSocket surface |
| 8. Frontend Completion | Milestone 7 | Full UI per Project.md |
| 9. Voice | Milestone 8 | Evidence-gated only |
| 10. Security | Milestone 8 | Evidence-gated only |

**Milestones 1-5**: Core daily-use system. Ship and use before building more.  
**Milestones 6-8**: Complete and harden what exists.  
**Milestones 9-10**: Add only when real usage demands it.